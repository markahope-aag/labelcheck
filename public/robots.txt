# Robots.txt for LabelCheck

# Allow all crawlers to access public pages
User-agent: *
Allow: /
Allow: /pricing
Allow: /share/*

# Disallow authenticated/private areas
Disallow: /dashboard
Disallow: /admin
Disallow: /analyze
Disallow: /history
Disallow: /team
Disallow: /settings
Disallow: /api/

# Disallow authentication pages (prevent crawlers from triggering auth flows)
Disallow: /sign-in
Disallow: /sign-up
Disallow: /accept-invitation

# Sitemap (if you create one in the future)
# Sitemap: https://yourdomain.com/sitemap.xml
